{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Assignment-2-17110053-Peeyush.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq1kG69nzeku",
        "colab_type": "text"
      },
      "source": [
        "# Estimating Maximum Linear Estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa_jSkzhzekv",
        "colab_type": "code",
        "outputId": "08253f11-b413-4f4d-d5f2-92d14c8793be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import requests\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "#Defining count function\n",
        "def count(z, n): #z --> text and n --> no of words at once\n",
        "    l = {}\n",
        "    for i in range(len(z) - n):\n",
        "        m = tuple([z[j] for j in range(i, i + n)])\n",
        "        if m not in l: l[m] = 1\n",
        "        else: l[m] += 1\n",
        "    return l\n",
        "            \n",
        "#Defining maximum likelihood estimator function \n",
        "def MLE(z, n):\n",
        "    l = count(z, n)\n",
        "    if n == 1: \n",
        "        for i in l: l[i] = l[i] / len(z)\n",
        "    else:\n",
        "        b = count(z, n - 1)\n",
        "        for i in l: l[i] = l[i] / b[i[:len(i)-1]]\n",
        "    return l\n",
        "\n",
        "#To get Gutenberg's text\n",
        "f = requests.get(\"http://www.gutenberg.org/files/31100/31100.txt\")\n",
        "f = f.text\n",
        "#Removing website urls and converting Mr., Mrs., Esq. to Mr, Mrs and Esq respectively\n",
        "content = re.sub(\"(http://[^%s]+|www.[^%s]+|\\n)\", \" \", f.lower())\n",
        "content = re.sub(\"Mr.\", \"Mr\", content)\n",
        "content = re.sub(\"Mrs.\", \"Mrs\", content)\n",
        "content = re.sub(\"[Ee]sq.\", \"esq\", content)\n",
        "\n",
        "#Tokenizing sentences\n",
        "e = sent_tokenize(content)\n",
        "\n",
        "#Tokenizing words\n",
        "z = []\n",
        "for i in range(int(len(e) * 0.8)):\n",
        "    z.append(\"<s>\")\n",
        "    z.extend(word_tokenize(e[i][:-1]))\n",
        "    z.append(\"</s>\")\n",
        "\n",
        "test = []\n",
        "for i in range(int(len(e) * 0.8), len(e)):\n",
        "    test.append(\"<s>\")\n",
        "    test.extend(word_tokenize(e[i][:-1]))\n",
        "    test.append(\"</s>\")\n",
        "#Size of Vocabulary\n",
        "vocabulary = len(set(z))\n",
        "\n",
        "#Unigram, Bigram, Trigram and Quadgram MLEs\n",
        "unigram = MLE(z, 1)\n",
        "bigram = MLE(z, 2)\n",
        "trigram = MLE(z, 3)\n",
        "quadgram = MLE(z, 4)\n",
        "print(len(unigram), len(bigram), len(trigram), len(quadgram))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "15706 178401 451627 640444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8fuH6wXzekz",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIVyXaL3zek0",
        "colab_type": "code",
        "outputId": "2c1b9161-a113-49ff-8b7b-b4233d4f5231",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import numpy\n",
        "import random\n",
        "\n",
        "def Generator(t, s):\n",
        "    #Initialization\n",
        "    b = [] #list containing tuples of tuples and its MLE\n",
        "    for i in t: b.append((i, t[i]))\n",
        "    n = len(list(t.keys())[0])\n",
        "    \n",
        "    #Initially starting with <s>\n",
        "    alpha = list(random.choice([i for i in t if i[0] == \"<s>\"])) #Code equation ---- 1\n",
        "    \n",
        "    #Code equation ---- 1 can be understood here\n",
        "    #a = []\n",
        "    #for i in t: \n",
        "    #    if i[0] == \"<s>\": a.append(i)\n",
        "    #b = random.choice(a)\n",
        "    #alpha = list(b)\n",
        "    \n",
        "    #For unigram    \n",
        "    if n == 1:\n",
        "        a = numpy.random.multinomial(5, [i[1] for i in b], size = s)\n",
        "        for i in range(len(a)):\n",
        "            alpha.append(b[random.choice(numpy.where(a[i] == numpy.amax(a[i])))[0]][0][0]) #Code equation ---- 2\n",
        "        return \" \".join(alpha)\n",
        "    \n",
        "    #Code equation ---- 2 can be understood here\n",
        "    #d = numpy.where(a[i] == numpy.amax(a[i]))\n",
        "    #c = random.choice(d[0])\n",
        "    #alpha.append(b[c][0][0])\n",
        "    \n",
        "    #For Bigram, Trigram, Quadgram\n",
        "    elif n >= 2:\n",
        "        for i in range(s):\n",
        "            m = [(b[i][0], b[i][1]) for i in range(len(b)) if list(b[i][0])[0:n - 1] == alpha[len(alpha) - n + 1:]]\n",
        "            #m = [(tuple, probability), .....] m contains tuples which are in alpha[len(alpha) - n + 1]\n",
        "            a = numpy.random.multinomial(5, [i[1] for i in m], size = 1)\n",
        "                      \n",
        "            beta = numpy.where(a[0] == numpy.amax(a[0])) #beta = (array([......], dtype = int64),)\n",
        "            #beta contains indies of maximum probabilities in a[0]\n",
        "            gamma = [m[i] for i in beta[0]] #gamma = [(values, probability)] are the values that have max probability\n",
        "            delta = random.choice(gamma)[0] #delta = (random choice, probability)\n",
        "            alpha.append(delta[n - 1])\n",
        "            \n",
        "        return \" \".join(alpha)\n",
        "print(\"Unigram text:\")\n",
        "print(Generator(unigram, 100))\n",
        "print()\n",
        "print(\"Bigram text:\")\n",
        "print(Generator(bigram, 100))\n",
        "print()\n",
        "print(\"Trigram text:\")\n",
        "print(Generator(trigram, 100))\n",
        "print()\n",
        "print(\"Quadgram text:\")\n",
        "print(Generator(quadgram, 100))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigram text:\n",
            "<s> , , </s> , <s> and , of , is the many , , and <s> <s> , was of , ; , at the of <s> <s> , is , and </s> the is of </s> with the <s> to , it 's the , for , of he <s> to was in <s> the <s> <s> in the and ; as , of <s> <s> of <s> 's the 's <s> a , </s> ; the <s> and <s> a of of </s> to and <s> <s> <s> </s> no lady , , of , it and so\n",
            "\n",
            "Bigram text:\n",
            "<s> 'nobody was lady russell 's opinion of which made her end of a roll , why she was all the first object of humiliation to believe . </s> <s> `` there could justify inquiry , ere long , `` going to give you must be sure , and animate and miss crawford and probably -- and though i remember thinking of the same </s> <s> if she had done </s> <s> it just as to sit down pulteney street , which reflected on the natural taste , only to such a proper time , and who have been in the dullness\n",
            "\n",
            "Trigram text:\n",
            "<s> even their mother , who was pouring out his watch -- '' `` he is a civil message ; but it was of no use in holding hats and bonnets </s> <s> `` the pianoforte , and who , late and dark and dirty </s> <s> `` of his consequence increased with what she did not press her to an alarm not wholly without apprehension </s> <s> she is likely to give up music . </s> <s> `` i am sure you were a most aching heart , and must now be quiet </s> <s> there will be surprised , and as\n",
            "\n",
            "Quadgram text:\n",
            "<s> as nothing was really left for the decision of mrs. price , beyond a doubt , and you must have been a doubt </s> <s> nature may have done wrong with regard to the girls it certainly is no proof of their fathers beauty , for she will not know him herself . </s> <s> `` only think of his past declarations , i give you leave to like him </s> <s> `` here is something still more valuable , i mean to call them the arbiters of good-breeding , the regulators of refinement and courtesy , the masters of the ceremonies introduced\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SoOEcHszek2",
        "colab_type": "text"
      },
      "source": [
        "# Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx-m7Ri5zek3",
        "colab_type": "code",
        "outputId": "73482efc-e8bb-4608-b6e2-a5822fdc5155",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import math\n",
        "#Defining Add - 1 Perplexity function\n",
        "def AddkPerplexity(z, test, n, k): #gram --> which ngram (number)\n",
        "    perplexity = 0\n",
        "    vocabulary = len(set(z))\n",
        "    l = count(z, n)\n",
        "    if n == 1:\n",
        "        for j in test:\n",
        "            if tuple([j]) in l: perplexity += math.log((vocabulary*k + len(z)) / (l[tuple([j])] + k), 10) \n",
        "            else: perplexity += math.log((vocabulary*k + len(z))/k, 10)\n",
        "    else: \n",
        "        b = count(z, n - 1)\n",
        "        for j in range(n, len(test)):\n",
        "            if tuple(test[j - n:j]) in l: perplexity += math.log(vocabulary*k + b[tuple(test[j - n:j - 1])] / (l[tuple(test[j - n:j])] + k), 10)\n",
        "            else: \n",
        "                if tuple(test[j - n:j - 1]) in b: perplexity += math.log((vocabulary*k + b[tuple(test[j - n:j - 1])])/k, 10)\n",
        "                else: perplexity += math.log((vocabulary*k + len(z))/k, 10)\n",
        "    return 10**(perplexity/len(test))\n",
        "print(AddkPerplexity(z, test, 1, 1))\n",
        "print(AddkPerplexity(z, test, 2, 1))\n",
        "print(AddkPerplexity(z, test, 3, 1))\n",
        "print(AddkPerplexity(z, test, 4, 1))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "503.466217861336\n",
            "17806.84713984563\n",
            "32499.95755523519\n",
            "120819.50339911354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8olOyqBOzek5",
        "colab_type": "text"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51_4pdUszek5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "from keras.callbacks import ModelCheckpoint, LambdaCallback\n",
        "from keras.utils import np_utils\n",
        "import matplotlib.pyplot as plt\n",
        "import sys, io, random\n",
        "import numpy as np\n",
        "from keras.optimizers import RMSprop\n",
        "import numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0CZZZzWzek9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = content\n",
        "chars = sorted(list(set(text)))\n",
        "char_indices = dict((c, i) for i, c, in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYCl1ZbFAbF7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0f343081-ec28-4ffe-b5cc-4d773b027edd"
      },
      "source": [
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n",
        "\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb sequences: 1484477\n",
            "Vectorization...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYyJ_oAEW4D7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "98e94231-32eb-4d96-d1d7-ec25b78deba0"
      },
      "source": [
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "#optimizer = RMSprop(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqk8WaU-W8u6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xBsB5UvXAbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(700):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONChXIvgXDAp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6406f217-9416-4b6c-8d49-dc311a2046be"
      },
      "source": [
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=128,\n",
        "          epochs=5,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1484477/1484477 [==============================] - 934s 630us/step - loss: 1.6348\n",
            "\n",
            "----- Generating text after Epoch: 0\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \" is a nice book, and why should not i ca\"\n",
            " is a nice book, and why should not i can her father from a sure the same and her from the party with a man and her sister the some to be a man in the some and her father was a man of her father as the sime of the surpose of the could not be an the sister to her father of the parting the sister to the should have been the sure of the complied to her for her from the should be a same of her father who he had not the sister of the conside\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \" is a nice book, and why should not i ca\"\n",
            " is a nice book, and why should not i can her sister as the party with a little which he had he latter and so much the consiculation of miss call continges and see formice. she had the confiching to amore of the distress of reasous to the same this given the rematient the collicionally be any in this interest of him all the will he sell the perst to a most that the wasted the marting no sister the whole is is well and calceress the grea\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \" is a nice book, and why should not i ca\"\n",
            " were poor in miselboy. actemencustly have a\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \" is a nice book, and why should not i ca\"\n",
            " sweffil gearlat,\" s\n",
            "Epoch 2/5\n",
            "1484477/1484477 [==============================] - 929s 626us/step - loss: 1.4732\n",
            "\n",
            "----- Generating text after Epoch: 1\n",
            "----- diversity: 0.2\n",
            " they were n\"\n",
            " they were no scare that i have been a most disprosent of the comparions of the same and the same to be consideration to the side of the same and said mr. what have been any married her family in the course to the most all the sister was a sintered that it was a most disappointed the sensible as she was a few dound of the attention of the the same of the same and the same and say, i am some than her all the f\n",
            "----- diversity: 0.5\n",
            " they were n\"\n",
            " they were no liken the lady for it. that i marner lang she was in the dear some time to the being her formed the descrition of the house found her own constant of the other for mr. knightley certainty that the same to mrs. jennings, and the happiness, and she was a well and start the house to be surry to feel other, and she was any beard her sensitire of the time to the family all the parted for the marned b\n",
            "----- diversity: 1.0\n",
            " they were n\"\n",
            " no hour\n",
            "----- diversity: 1.2\n",
            " they were n\"\n",
            " could me. i would to your. thuter loo\n",
            "Epoch 3/5\n",
            "1484477/1484477 [==============================] - 928s 625us/step - loss: 1.3976\n",
            "\n",
            "----- Generating text after Epoch: 2\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"m again. there is something in the sound\"\n",
            "m again. there is something in the sound and so much to be dear and discompliation of her and marianne so sure i am sure i am sure i think it was not a few more than the considerations of the particularly and so much to be an any more the part of the part which had been to be any of the particularly and suppose of the thing of the rest of the world with the compliment of the part of his with the state of the dear and hardly so much and \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"m again. there is something in the sound\"\n",
            "m again. there is something in the sound as the look at the particularly and some to see, a man which had a little sirce the first to return of some are she had not the door of instantly of marianne and so it on the some feelings. with a latter to his sive into the most have good to the more than the whole of the discomplination of his story to see his own sold himself which which he would have been any sisters to her of the same and af\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"m again. there is something in the sound\"\n",
            " benerion as had consider thoses; but which old you really thought rustent of the pleasury to her; b\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"m again. there is something in the sound\"\n",
            " \"picclives, he hashker in fear my mean ingendary \n",
            "Epoch 4/5\n",
            "1484477/1484477 [==============================] - 935s 630us/step - loss: 1.3537\n",
            "\n",
            "----- Generating text after Epoch: 3\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"y, \"his observations may have extended t\"\n",
            "y, \"his observations may have extended to be any thing to be a sincere of the proposed to be a sire to her and surprise of her sister with the sister the some more the distance of the proposed to the street and the street and with the street and the particular to be sorry to be sure it was not the street as the street as the same of the streets of the street as he was a sensible as to the street of the street and the proper in the stree\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"y, \"his observations may have extended t\"\n",
            "y, \"his observations may have extended to be sorry to the same tour any companion with i have degle starrity of his resting to her some first a subject and miss darcy as to the present a day, not it the sincere was preasure was remained to the lady miss gratted to be done so grather of the sturk of company and hardly like carries as i think of her at the distance with the sooner to be all the particular to be sorry to herself to be any \n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"y, \"his observations may have extended t\"\n",
            " are so stor for it by; as you was\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"y, \"his observations may have extended t\"\n",
            " account, grever; \"and sh\n",
            "Epoch 5/5\n",
            "  12544/1484477 [..............................] - ETA: 15:42 - loss: 1.3355"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-fc2cc65059d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           callbacks=[print_callback])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9_gjMHJXEx6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "47aaadd2-4271-42f7-a95b-11a63bc5d7c5"
      },
      "source": [
        "n_chars = len(text)\n",
        "n_vocab = len(chars)\n",
        "\n",
        "\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(n_chars - seq_length):\n",
        "  seq_in = text[i: i + seq_length]\n",
        "  seq_out = text[i + seq_length]\n",
        "  dataX.append([char_indices[char] for char in seq_in])\n",
        "  dataY.append(char_indices[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "X = X / float(n_vocab)\n",
        "y = np_utils.to_categorical(dataY)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape(X.shape[1], X.shape[2]), return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation = \"softmax\"))\n",
        "model.load_weights(text)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
        "\n",
        "#Pick random seed\n",
        "print(\"Seed: \" + ' '.join([indices_char[value] for value in pattern]))\n",
        "\n",
        "#Generate Characters\n",
        "for i in range(700):\n",
        "  x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "  x = x / float(n_vocab)\n",
        "  prediction = model.predict(x, verbose =  0)\n",
        "  index = numpy.argmax(prediction)\n",
        "  result = indices_char[index]\n",
        "  seq_in = [indices_char[value] for value in pattern]\n",
        "  sys.stdout.write(result)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-b3320662f087>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'input_shape' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzoBgUcv557K",
        "colab_type": "text"
      },
      "source": [
        "The N-gram is fast and efficient for small datasets whereas RNN fails to do so. But given a large dataset, RNN is more efficient because it relies on all previous values of the dataset whereas n-gram relies only on past n values of the dataset which tells that the long term relation of a word/sentence cannot be captured in n-gram whereas in RNN and LSTM, it can be. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwkiS1Rh8CNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}